{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''数据读取与处理模块, only read daily data'''\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "filepath = '20200101-20201016.csv'\n",
    "\n",
    "daily_stock_price_columns = ['index', 'time', 'code', 'open', 'close', 'low', 'high', 'volume', 'money', 'factor',\n",
    "                             'high_limit', 'low_limit', 'avg', 'pre_close', 'paused', 'open_interest']\n",
    "daily_stock_price_column_dtype = {\n",
    "    'index': float, 'time': str, 'code': str, 'open': float, 'close': float, 'low': float, 'high': float,\n",
    "    'volume': float, 'money': float, 'factor': float, 'high_limit': float, 'low_limit': float, 'avg': float,\n",
    "    'pre_close': float, 'paused': str, 'open_interest': str\n",
    "}\n",
    "\n",
    "df = pd.read_csv(filepath, header=None, parse_dates=['time'], na_values='\\\\N', names=daily_stock_price_columns, dtype=daily_stock_price_column_dtype)\n",
    "df = df.fillna(1e-10)\n",
    "\n",
    "df['pct_change'] = (df.close - df.pre_close)/df.close * 100\n",
    "df['daily_status'] = df['pct_change'].map(lambda x: 0 if x < 0 else 1)\n",
    "\n",
    "daily_stock_price_columns_normalized = df.groupby('code').apply(\n",
    "    lambda x: x.sort_values(by='time', ascending=True)[['open', 'close', 'low', 'high', 'volume', 'money', 'daily_status']].values)\n",
    "\n",
    "\n",
    "def split_data_with_gap(array, data_len, data_gap):\n",
    "    '''Input should be an array, return a +1 dimension array.'''\n",
    "    array_slice_list = [array[i*data_gap:(i*data_gap+data_len)]\n",
    "                        for i in range((len(array)-data_len)//data_gap)]\n",
    "    return np.array(array_slice_list)\n",
    "\n",
    "\n",
    "daily_stock_price_columns_normalized = daily_stock_price_columns_normalized.map(\n",
    "    lambda x: split_data_with_gap(x, 30, 10))\n",
    "daily_stock_price_columns_normalized = daily_stock_price_columns_normalized.values\n",
    "\n",
    "daily_stock_price_columns_normalized_array = []\n",
    "for x in daily_stock_price_columns_normalized:\n",
    "    if len(x) != 0:\n",
    "        for y in x:\n",
    "            daily_stock_price_columns_normalized_array.append(y)\n",
    "daily_stock_price_columns_normalized_array = np.array(\n",
    "    daily_stock_price_columns_normalized_array)\n",
    "\n",
    "print(daily_stock_price_columns_normalized_array.shape)\n",
    "with open(filepath+'.pickle', 'wb') as fw:\n",
    "    pickle.dump(daily_stock_price_columns_normalized_array, fw)\n",
    "    print('successfuly saved {}'.format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''数据读取与处理模块-- combine serveral dataframe'''\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "daily_stock_filepath = '20200101-20201016.csv'\n",
    "daily_stock_price_columns = ['index', 'date', 'sec_code', 'open', 'close', 'low', 'high', 'volume', 'money', 'factor',\n",
    "                             'high_limit', 'low_limit', 'avg', 'pre_close', 'paused', 'open_interest']\n",
    "daily_stock_price_column_dtype = {\n",
    "    'index': float, 'date': str, 'sec_code': str, 'open': float, 'close': float, 'low': float, 'high': float,\n",
    "    'volume': float, 'money': float, 'factor': float, 'high_limit': float, 'low_limit': float, 'avg': float,\n",
    "    'pre_close': float, 'paused': str, 'open_interest': str\n",
    "}\n",
    "\n",
    "mtss_data_filepath = '20200101-20201016.csv'\n",
    "mtss_data_columns = ['date', 'sec_code', 'fin_value', 'fin_buy_value', 'fin_refund_value','sec_value', 'sec_sell_value',\n",
    "                    'sec_refund_value', 'fin_sec_value']\n",
    "mtss_data_dtype = {\n",
    "    'date': str, 'sec_code':str, 'fin_value':float, 'fin_buy_value':float, 'fin_refund_value':float,\n",
    "    'sec_value':float, 'sec_sell_value':float, 'sec_refund_value':float, 'fin_sec_value':float\n",
    "}\n",
    "\n",
    "call_auction_filepath = '20200101-20201016.csv'\n",
    "call_aution_data_columns = ['sec_code', 'date', 'current', 'volume', 'money', 'a1_p', 'a1_v', 'a2_p', 'a2_v', 'a3_p',\n",
    "                           'a3_v', 'a4_p', 'a4_v', 'a5_p', 'a5_v', 'b1_p', 'b1_v', 'b2_p', 'b2_v', 'b3_p', 'b3_v',\n",
    "                           'b4_p', 'b4_v', 'b5_p', 'b5_v']\n",
    "call_aution_data_dtype = {\n",
    "    'sec_code': str, 'date': str, 'current': float, 'volume': float, 'money': float, 'a1_p': float, 'a1_v': float, \n",
    "    'a2_p': float, 'a2_v': float, 'a3_p': float, 'a3_v': float, 'a4_p': float, 'a4_v': float, 'a5_p': float,\n",
    "    'a5_v': float, 'b1_p': float, 'b1_v': float, 'b2_p': float, 'b2_v': float, 'b3_p': float, 'b3_v': float,\n",
    "    'b4_p': float, 'b4_v': float, 'b5_p': float, 'b5_v': float\n",
    "}\n",
    "\n",
    "money_flow_filepath = '20200101-20201016.csv'\n",
    "money_flow_data_columns = ['date', 'sec_code', 'change_pct', 'net_amount_main', 'net_pct_main', 'net_amount_xl', 'net_pct_xl',\n",
    "                  'net_amount_l', 'net_pct_l', 'net_amount_m', 'net_pct_m', 'net_amount_s', 'net_pct_s']\n",
    "\n",
    "money_flow_dtype = {\n",
    "    'date': str, 'sec_code': str, 'change_pct': float, 'net_amount_main': float, 'net_pct_main': float, \n",
    "    'net_amount_xl': float, 'net_pct_xl': float, 'net_amount_l': float, 'net_pct_l': float, 'net_amount_m': float,\n",
    "    'net_pct_m': float, 'net_amount_s': float, 'net_pct_s': float\n",
    "}\n",
    "\n",
    "daily_df = pd.read_csv(daily_stock_filepath, header=None, na_values='\\\\N', names=daily_stock_price_columns, dtype=daily_stock_price_column_dtype).fillna(1e-10)\n",
    "mtss_df = pd.read_csv(mtss_data_filepath, header=None, na_values='\\\\N', names=mtss_data_columns, dtype=mtss_data_dtype).fillna(1e-10)\n",
    "money_flow_df = pd.read_csv(money_flow_filepath, header=None, na_values='\\\\N', names=money_flow_data_columns, dtype=money_flow_dtype).fillna(1e-10)\n",
    "call_auction_df = pd.read_csv(call_auction_filepath, header=None, na_values='\\\\N', names=call_aution_data_columns, dtype=call_aution_data_dtype).fillna(1e-10)\n",
    "\n",
    "combined_df = reduce(lambda x, y: pd.merge(x, y, how='outer', on=['sec_code', 'date']), [daily_df, mtss_df, money_flow_df, call_auction_df])\n",
    "\n",
    "combined_df.to_csv('combined_data.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-07T01:34:55.017Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Read combined data, split it to pieces.'''\n",
    "import pandas as pd \n",
    "import time\n",
    "import numpy as np \n",
    "import pickle \n",
    "import sklearn.preprocessing\n",
    "import sklearn.feature_selection\n",
    "\n",
    "# filepath = './combined_20180101-2020-10-26.csv'\n",
    "# combined_df = pd.read_csv(filepath, header=0).fillna(1e-10)  # Have alread convert other Nan to real NaN.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:47:17.315974Z",
     "start_time": "2020-10-27T15:38:56.165825Z"
    }
   },
   "outputs": [],
   "source": [
    "'''特征分析与筛选模块'''\n",
    "import pandas as pd \n",
    "import time\n",
    "import numpy as np \n",
    "import pickle \n",
    "import sklearn.preprocessing\n",
    "import sklearn.feature_selection\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import gen_features\n",
    "\n",
    "\n",
    "#################### 单 daily_stock模块 #####################\n",
    "# filepath = '20200101-20201016.csv'\n",
    "\n",
    "# daily_stock_price_columns = ['index', 'time', 'code', 'open', 'close', 'low', 'high', 'volume', 'money', 'factor',\n",
    "#                             'high_limit', 'low_limit', 'avg', 'pre_close', 'paused', 'open_interest']\n",
    "# daily_stock_price_column_dtype = {\n",
    "#     'index': float, 'time': str, 'code': str, 'open': float, 'close': float, 'low': float, 'high': float,\n",
    "#     'volume': float, 'money': float, 'factor': float, 'high_limit': float, 'low_limit': float, 'avg': float, \n",
    "#     'pre_close': float, 'paused': str, 'open_interest': str\n",
    "# }\n",
    "# df = pd.read_csv(filepath, header=None, parse_dates=['time'], na_values='\\\\N',\n",
    "#                  names=daily_stock_price_columns, dtype=daily_stock_price_column_dtype)\n",
    "# df = df.fillna(1e-10)\n",
    "# df['pct_change'] = (df.close - df.pre_close)/df.close * 100\n",
    "#################### 单 daily_stock模块 #####################\n",
    "\n",
    "#################### combined dataframe 模块 #####################\n",
    "filepath = './combined_20180101-2020-10-26.csv'\n",
    "combined_df = pd.read_csv(filepath, header=0).fillna(1e-10)  # Have alread convert other Nan to real NaN.\n",
    "\n",
    "columns_list = ['open', 'close', 'low', 'high', 'volume_x', 'money_x', 'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'open_interest', 'current', 'volume_y', 'money_y', 'a1_p', 'a1_v', 'a2_p', 'a2_v', 'a3_p', 'a3_v', 'a4_p', 'a4_v', 'a5_p', 'a5_v', 'b1_p', 'b1_v', 'b2_p', 'b2_v', 'b3_p', 'b3_v', 'b4_p', 'b4_v', 'b5_p', 'b5_v', 'fin_value', 'fin_buy_value', 'fin_refund_value', 'sec_value', 'sec_sell_value', 'sec_refund_value', 'fin_sec_value', 'change_pct', 'net_amount_main', 'net_pct_main', 'net_amount_xl', 'net_pct_xl', 'net_amount_l', 'net_pct_l', 'net_amount_m', 'net_pct_m', 'net_amount_s', 'net_pct_s']\n",
    "\n",
    "\n",
    "#### Conclusion: minmax, maxabs 方差较小，origial:方差较大 其他：方差固定1.01\n",
    "columns_list_list = [[x] for x in columns_list]\n",
    "feature_def = gen_features(\n",
    "    columns=columns_list_list,\n",
    "#     classes=[{'class': sklearn.feature_selection.VarianceThreshold, 'threshold': 0.5}],\n",
    "    classes=[sklearn.preprocessing.StandardScaler],\n",
    "#     classes=[None]\n",
    ")\n",
    "feature_def = feature_def + [(['date'], None, {}), (['sec_code'], None, {}), (['change_pct'], sklearn.preprocessing.Binarizer(threshold=0), {})]\n",
    "mapper = DataFrameMapper(feature_def, df_out=True)\n",
    "normalized_df = mapper.fit_transform(combined_df)\n",
    "\n",
    "normalized_df.to_csv('./combined_20180101-2020-10-26_normazlied.csv', header=True, index=False)\n",
    "\n",
    "# def variance_threshold_selector(data, threshold=0.5):\n",
    "#     selector = sklearn.feature_selection.VarianceThreshold(threshold)\n",
    "#     selector.fit(data)\n",
    "# #     return data[data.columns[selector.get_support(indices=True)]]\n",
    "#     return data.columns[selector.get_support(indices=True)].tolist()\n",
    "\n",
    "# columns_list = [[x] for x in columns_list]\n",
    "# feature_def = gen_features(\n",
    "#     columns=columns_list,\n",
    "#     classes=[{'class': sklearn.feature_selection.VarianceThreshold, 'threshold': 0.5}]\n",
    "# )\n",
    "# feature_selection_mapper = DataFrameMapper(feature_def, df_out=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T17:02:42.136053Z",
     "start_time": "2020-11-09T16:58:53.131103Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274477, 30, 53, 1)\n",
      "successfully dump the compressed split data.\n",
      "(54896, 29, 52, 1) (54896, 1)\n"
     ]
    }
   ],
   "source": [
    "###### build the model\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import keras\n",
    "import pandas as pd \n",
    "import pickle \n",
    "\n",
    "def dump_model(model, filepath):\n",
    "    with open(filepath, 'wb') as fw:\n",
    "        pickle.dump(model, fw)\n",
    "    print('dumping {} successfully...'.format(filepath))\n",
    "    \n",
    "def load_model(filepath):\n",
    "    with open(filepath, 'rb') as fr:\n",
    "        model = pickle.load(fr)\n",
    "    print('loading {} successfully...'.format(filepath))\n",
    "    return model\n",
    "\n",
    "def evaluate_with_threshold(predicted_y, y_true, threshold):\n",
    "    y_true = np.squeeze(y_true)\n",
    "    print(predicted_y.shape)\n",
    "    predicted_y = predicted_y.tolist()\n",
    "    predicted_y_with_threshold = []\n",
    "    for x in predicted_y:\n",
    "        if x[0] >= threshold:\n",
    "            predicted_y_with_threshold.append(0)\n",
    "        elif x[1] >= threshold:\n",
    "            predicted_y_with_threshold.append(1)\n",
    "        else:\n",
    "            predicted_y_with_threshold.append(100)\n",
    "    predicted_y_with_threshold = np.array(predicted_y_with_threshold)\n",
    "    \n",
    "    y_equal = y_true[predicted_y_with_threshold == y_true]\n",
    "    \n",
    "    from collections import Counter\n",
    "    test_y_counter = Counter(y_equal)\n",
    "    print('Inside prediction data: 1: {}, 0: {}, not fulfill threshold: {}'.format(test_y_counter.get(1), test_y_counter.get(0), test_y_counter.get(100)))\n",
    "    print('With threshold, out of {} examples, {} are right. acc: {}'.format(len(y_true), len(y_equal), 100. * len(y_equal)/len(y_true)))\n",
    "\n",
    "            \n",
    "columns_list = ['sec_code', 'date', 'open', 'close', 'low', 'high', 'volume_x', 'money_x', 'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'open_interest', 'current', 'volume_y', 'money_y', 'a1_p', 'a1_v', 'a2_p', 'a2_v', 'a3_p', 'a3_v', 'a4_p', 'a4_v', 'a5_p', 'a5_v', 'b1_p', 'b1_v', 'b2_p', 'b2_v', 'b3_p', 'b3_v', 'b4_p', 'b4_v', 'b5_p', 'b5_v', 'fin_value', 'fin_buy_value', 'fin_refund_value', 'sec_value', 'sec_sell_value', 'sec_refund_value', 'fin_sec_value', 'net_amount_main', 'net_pct_main', 'net_amount_xl', 'net_pct_xl', 'net_amount_l', 'net_pct_l', 'net_amount_m', 'net_pct_m', 'net_amount_s', 'net_pct_s', 'change_pct']\n",
    "calculation_columns_list = ['open', 'close', 'low', 'high', 'volume_x', 'money_x', 'factor', 'high_limit', 'low_limit', 'avg', 'pre_close', 'open_interest', 'current', 'volume_y', 'money_y', 'a1_p', 'a1_v', 'a2_p', 'a2_v', 'a3_p', 'a3_v', 'a4_p', 'a4_v', 'a5_p', 'a5_v', 'b1_p', 'b1_v', 'b2_p', 'b2_v', 'b3_p', 'b3_v', 'b4_p', 'b4_v', 'b5_p', 'b5_v', 'fin_value', 'fin_buy_value', 'fin_refund_value', 'sec_value', 'sec_sell_value', 'sec_refund_value', 'fin_sec_value', 'net_amount_main', 'net_pct_main', 'net_amount_xl', 'net_pct_xl', 'net_amount_l', 'net_pct_l', 'net_amount_m', 'net_pct_m', 'net_amount_s', 'net_pct_s', 'change_pct']\n",
    "\n",
    "def split_data_with_gap(array, data_len, data_gap):\n",
    "    '''Input should be an array, return a +1 dimension array.'''\n",
    "    array_slice_list = [array[i*data_gap:(i*data_gap+data_len)]\n",
    "                        for i in range((len(array)-data_len)//data_gap)]\n",
    "    return np.array(array_slice_list)\n",
    "\n",
    "filepath = 'combined_20180101-2020-10-26_normazlied.csv'\n",
    "combined_df = pd.read_csv(filepath, header=0).fillna(1e-10)\n",
    "combined_df = combined_df[columns_list]\n",
    "\n",
    "combined_df_normalized = combined_df.groupby('sec_code').apply(\n",
    "    lambda x: x.sort_values(by='date', ascending=True)[calculation_columns_list].values)\n",
    "\n",
    "combined_df_normalized = combined_df_normalized.map(\n",
    "    lambda x: split_data_with_gap(x, 30, 10))\n",
    "combined_df_normalized = combined_df_normalized.values\n",
    "\n",
    "combined_df_normalized_array = []\n",
    "for x in combined_df_normalized:\n",
    "    if len(x) != 0:\n",
    "        for y in x:\n",
    "            combined_df_normalized_array.append(y)\n",
    "combined_df_normalized_array = np.expand_dims(np.array(\n",
    "    combined_df_normalized_array), axis=-1)\n",
    "\n",
    "binary_tag_function = np.vectorize(lambda x: 1 if x >=0 else 0)\n",
    "combined_df_normalized_array[:, :, -1, 0] = binary_tag_function(combined_df_normalized_array[:, :, -1, 0])\n",
    "\n",
    "# print(combined_df_normalized_array)\n",
    "print(combined_df_normalized_array.shape)\n",
    "\n",
    "data_train, data_test = train_test_split(combined_df_normalized_array, test_size=0.2)\n",
    "\n",
    "#compress the file to disk\n",
    "import gzip \n",
    "with gzip.GzipFile('split_combined_data.zip', 'w') as fw:\n",
    "    pickle.dump((data_train, data_test), fw)\n",
    "    print('successfully dump the compressed split data.')\n",
    "\n",
    "test_x, test_y = data_test[:, :-1, :-1, :], data_test[:, -1, -1, :]\n",
    "print(test_x.shape, test_y.shape)\n",
    "train_x, train_y = data_train[:, :-1, :-1, :], data_train[:, -1, -1, :]\n",
    "\n",
    "# from collections import Counter\n",
    "# test_y_counter = Counter(test_y[:, 0])\n",
    "# train_y_counter = Counter(train_y[:, 0])\n",
    "# print('Inside training data: 1: {}, 0: {}'.format(train_y_counter.get(1), train_y_counter.get(0)))\n",
    "# print('Inside testing data: 1: {}, 0: {}'.format(test_y_counter.get(1), test_y_counter.get(0)))\n",
    "\n",
    "# #---------- Using CNN model for predction------------\n",
    "# # input_layer = Input(shape=(29, 52, 1))\n",
    "# # x = BatchNormalization(axis=-2)(input_layer)\n",
    "# # x = Conv2D(filters=32, kernel_size=(8, 6), strides=(1, 6))(x)\n",
    "# # x = MaxPooling2D(pool_size=(2, 1), strides=(1, 1))(x)\n",
    "# # print(x.shape)\n",
    "# # x = Conv2D(filters=16, kernel_size=(8, 1), strides=(1, 1))(x)\n",
    "# # x = MaxPooling2D(pool_size=(8, 1), strides=(2, 1))(x)\n",
    "# # x = Flatten()(x)\n",
    "# # output_layer = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# # model = keras.Model(inputs=input_layer, outputs=output_layer, name='stock_model')\n",
    "# # model.summary()\n",
    "\n",
    "# # model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'],\n",
    "# #              optimizer=keras.optimizers.Adam(lr=0.01, decay=0.9))\n",
    "\n",
    "# # model_path = 'eacy_model_2sorftmax'\n",
    "# # tensorboard_path = model_path + '_tensorboard'\n",
    "# # model_checkpoint_callback = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n",
    "# # early_stopping_callback = keras.callbacks.EarlyStopping(patience=32)\n",
    "# # tensorboard_callback = keras.callbacks.TensorBoard(tensorboard_path)\n",
    "\n",
    "# # # model.fit(train_x, train_y, batch_size=32, epochs=100, validation_split=0.2, \n",
    "# # #          callbacks=[model_checkpoint_callback, early_stopping_callback, tensorboard_callback])\n",
    "\n",
    "# # model = keras.models.load_model(model_path)\n",
    "\n",
    "# # # model.evaluate(test_x, test_y)\n",
    "# # # print('evaluate train data:')\n",
    "# # # model.evaluate(train_x, train_y)\n",
    "\n",
    "# # print('for training data:')\n",
    "# # evaluate_with_threshold(model, train_x, 0.5, train_y)\n",
    "# # evaluate_with_threshold(model, train_x, 0.6, train_y)\n",
    "# # evaluate_with_threshold(model, train_x, 0.7, train_y)\n",
    "# # evaluate_with_threshold(model, train_x, 0.8, train_y)\n",
    "\n",
    "# # print('for testing data:')\n",
    "# # evaluate_with_threshold(model, test_x, 0.5, test_y)\n",
    "# # evaluate_with_threshold(model, test_x, 0.6, test_y)\n",
    "# # evaluate_with_threshold(model, test_x, 0.7, test_y)\n",
    "# # evaluate_with_threshold(model, test_x, 0.8, test_y)\n",
    "# #---------- Using CNN model for predction------------\n",
    "\n",
    "# #---------- Using xgboost for predction------------\n",
    "# import xgboost\n",
    "# train_x_2_dim = train_x.reshape((train_x.shape[0], -1))\n",
    "# train_y_2_dim = np.squeeze(train_y)\n",
    "# test_x_2_dim = test_x.reshape((test_x.shape[0], -1))\n",
    "# test_y_2_dim = np.squeeze(test_y)\n",
    "\n",
    "# xgboost_model = xgboost.XGBClassifier(n_estimators=100, max_depth=10, learning_rate=0.1, verbosity=1, booster='gbtree',\n",
    "#                                      n_jobs=10)\n",
    "# xgboost_model.fit(train_x_2_dim, train_y_2_dim, eval_set=[(test_x_2_dim, test_y_2_dim)])\n",
    "\n",
    "# print('saving model')\n",
    "# xgboost_model.save_model('stock_binary_classification_xgboost.model')\n",
    "\n",
    "# # evaluate the model\n",
    "# eval_results = xgboost_model.evals_result()\n",
    "# print(eval_results)\n",
    "\n",
    "# test_predict_prob = xgboost_model.predict_proba(test_x_2_dim)\n",
    "# print(test_predict_probt)\n",
    "#---------- Using xgboost for predction------------\n",
    "\n",
    "#---------- Using random forest for predction------------\n",
    "# train_x_2_dim = train_x.reshape((train_x.shape[0], -1))\n",
    "# train_y_2_dim = np.squeeze(train_y)\n",
    "# test_x_2_dim = test_x.reshape((test_x.shape[0], -1))\n",
    "# test_y_2_dim = np.squeeze(test_y)\n",
    "# filepath = 'stock_binary_classification_random_forest.model'\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # random_forest_model = RandomForestClassifier(n_estimators=20, n_jobs=10)\n",
    "# # random_forest_model.fit(train_x_2_dim, train_y_2_dim)\n",
    "# # dump_model(random_forest_model, filepath)\n",
    "\n",
    "# random_forest_model = load_model(filepath)\n",
    "\n",
    "# print(random_forest_model.feature_importances_)\n",
    "\n",
    "# print('evaluating...')\n",
    "# print(random_forest_model.score(test_x_2_dim, test_y_2_dim))\n",
    "\n",
    "# print('evaluate traing data:')\n",
    "# train_y_predict = random_forest_model.predict_proba(train_x_2_dim)\n",
    "# evaluate_with_threshold(train_y_predict, train_y_2_dim, .7)\n",
    "\n",
    "# print('evaluate testing data:')\n",
    "# test_y_predict = random_forest_model.predict_proba(test_x_2_dim)\n",
    "# evaluate_with_threshold(test_y_predict, test_y_2_dim, 0.7)\n",
    "#---------- Using random forest for predction------------\n",
    "\n",
    "#---------- Using GBDT for predction------------\n",
    "# train_x_2_dim = train_x.reshape((train_x.shape[0], -1))\n",
    "# train_y_2_dim = np.squeeze(train_y)\n",
    "# test_x_2_dim = test_x.reshape((test_x.shape[0], -1))\n",
    "# test_y_2_dim = np.squeeze(test_y)\n",
    "# filepath = 'stock_binary_classification_gbdt.model'\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# gbdt_model = GradientBoostingClassifier(learning_rate=0.1, n_estimators=20)\n",
    "# gbdt_model.fit(train_x_2_dim, train_y_2_dim)\n",
    "# dump_model(gbdt_model, filepath)\n",
    "\n",
    "# gbdt_model = load_model(filepath)\n",
    "\n",
    "# print(gbdt_model.feature_importances_)\n",
    "\n",
    "# print('evaluating...')\n",
    "# print(gbdt_model.score(test_x_2_dim, test_y_2_dim))\n",
    "\n",
    "# print('evaluate traing data:')\n",
    "# train_y_predict = gbdt_model.predict_proba(train_x_2_dim)\n",
    "# evaluate_with_threshold(train_y_predict, train_y_2_dim, .7)\n",
    "\n",
    "# print('evaluate testing data:')\n",
    "# test_y_predict = gbdt_model.predict_proba(test_x_2_dim)\n",
    "# evaluate_with_threshold(test_y_predict, test_y_2_dim, 0.7)\n",
    "#---------- Using GBDT for predction------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
